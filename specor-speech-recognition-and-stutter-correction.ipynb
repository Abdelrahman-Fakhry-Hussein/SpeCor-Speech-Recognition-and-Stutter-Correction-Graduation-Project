{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "#! python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev\n",
    "#import torch_xla\n",
    "# imports pytorch\n",
    "#import torch\n",
    "\n",
    "# imports the torch_xla package\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "#device = xm.xla_device()\n",
    "#device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data and Preprocessing & Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:22.312169Z",
     "iopub.status.busy": "2024-03-02T00:03:22.311807Z",
     "iopub.status.idle": "2024-03-02T00:03:22.607341Z",
     "shell.execute_reply": "2024-03-02T00:03:22.606569Z",
     "shell.execute_reply.started": "2024-03-02T00:03:22.312139Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:36.081456Z",
     "iopub.status.busy": "2024-03-02T00:03:36.080989Z",
     "iopub.status.idle": "2024-03-02T00:03:37.741642Z",
     "shell.execute_reply": "2024-03-02T00:03:37.740745Z",
     "shell.execute_reply.started": "2024-03-02T00:03:36.08141Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_csv_files(root_folder):\n",
    "    csv_count = 0\n",
    "\n",
    "    # Loop over all folders and files in the root_folder\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                csv_count += 1\n",
    "\n",
    "    return csv_count\n",
    "\n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\n",
    "csv_count = count_csv_files(root_folder)\n",
    "\n",
    "if csv_count > 0:\n",
    "    print(f\"Number of .csv files found: {csv_count}\")\n",
    "else:\n",
    "    print(\"No .csv files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:37.742932Z",
     "iopub.status.busy": "2024-03-02T00:03:37.742641Z",
     "iopub.status.idle": "2024-03-02T00:03:37.748404Z",
     "shell.execute_reply": "2024-03-02T00:03:37.747553Z",
     "shell.execute_reply.started": "2024-03-02T00:03:37.742907Z"
    }
   },
   "outputs": [],
   "source": [
    "def insert_audio_clip(background, audio_clip):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the\n",
    "    audio segment does not overlap with existing segments.\n",
    "\n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording.\n",
    "    audio_clip -- the audio clip to be inserted/overlaid.\n",
    "    previous_segments -- times where audio segments have already been placed\n",
    "\n",
    "    Returns:\n",
    "    new_background -- the updated background audio\n",
    "    \"\"\"\n",
    "    background -= 25\n",
    "    # Get the duration of the audio clip in ms\n",
    "    segment_ms = len(audio_clip)\n",
    "\n",
    "\n",
    "    new_background = background.overlay(audio_clip, position = 0)\n",
    "\n",
    "    return new_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:37.751284Z",
     "iopub.status.busy": "2024-03-02T00:03:37.750948Z",
     "iopub.status.idle": "2024-03-02T00:03:37.76463Z",
     "shell.execute_reply": "2024-03-02T00:03:37.763842Z",
     "shell.execute_reply.started": "2024-03-02T00:03:37.751254Z"
    }
   },
   "outputs": [],
   "source": [
    "Out_Noise_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:37.766263Z",
     "iopub.status.busy": "2024-03-02T00:03:37.765911Z",
     "iopub.status.idle": "2024-03-02T00:03:42.242395Z",
     "shell.execute_reply": "2024-03-02T00:03:42.241433Z",
     "shell.execute_reply.started": "2024-03-02T00:03:37.766233Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "def find_min_speech_length_and_path(root_folder,Out_Noise_data):\n",
    "    min_length = float('inf')  # Set initial value to positive infinity\n",
    "    min_path = None\n",
    "    \n",
    "    # Loop over all folders and files in the root_folder\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.lower().endswith('.wav'):\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                audio = AudioSegment.from_wav(file_path)\n",
    "                \n",
    "                Out_Noise_data.append(audio)\n",
    "\n",
    "                # Open the wav file using sf\n",
    "                with sf.SoundFile(file_path, 'r') as file:\n",
    "                    # Get the duration in seconds\n",
    "                    duration_seconds = len(file) / file.samplerate\n",
    "\n",
    "                    # Convert to milliseconds\n",
    "                    duration_ms = int(duration_seconds * 1000)\n",
    "\n",
    "                    # Update min_length and min_path if the current speech length is smaller\n",
    "                    if duration_ms < min_length:\n",
    "                        min_length = duration_ms\n",
    "                        min_path = file_path\n",
    "\n",
    "    return Out_Noise_data, min_length, min_path\n",
    "\n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/noise-data-set/noise_train'\n",
    "Out_Noise_data,min_speech_length, min_path = find_min_speech_length_and_path(root_folder,Out_Noise_data)\n",
    "\n",
    "if min_path:\n",
    "    print(f\"The minimum speech length is {min_speech_length} milliseconds, found in file: {min_path}\")\n",
    "else:\n",
    "    print(\"No .wav files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:42.24383Z",
     "iopub.status.busy": "2024-03-02T00:03:42.243521Z",
     "iopub.status.idle": "2024-03-02T00:03:42.251013Z",
     "shell.execute_reply": "2024-03-02T00:03:42.250095Z",
     "shell.execute_reply.started": "2024-03-02T00:03:42.243804Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Out_Noise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:42.25289Z",
     "iopub.status.busy": "2024-03-02T00:03:42.252541Z",
     "iopub.status.idle": "2024-03-02T00:03:43.084479Z",
     "shell.execute_reply": "2024-03-02T00:03:43.083502Z",
     "shell.execute_reply.started": "2024-03-02T00:03:42.252859Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "def find_min_speech_length_and_path(root_folder,Out_Noise_data):\n",
    "    min_length = float('inf')  # Set initial value to positive infinity\n",
    "    min_path = None\n",
    "    \n",
    "    # Loop over all folders and files in the root_folder\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.lower().endswith('.wav'):\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                audio = AudioSegment.from_wav(file_path)\n",
    "                Out_Noise_data.append(audio)\n",
    "\n",
    "                # Open the wav file using sf\n",
    "                with sf.SoundFile(file_path, 'r') as file:\n",
    "                    # Get the duration in seconds\n",
    "                    duration_seconds = len(file) / file.samplerate\n",
    "\n",
    "                    # Convert to milliseconds\n",
    "                    duration_ms = int(duration_seconds * 1000)\n",
    "\n",
    "                    # Update min_length and min_path if the current speech length is smaller\n",
    "                    if duration_ms < min_length:\n",
    "                        min_length = duration_ms\n",
    "                        min_path = file_path\n",
    "\n",
    "    return Out_Noise_data, min_length, min_path\n",
    "\n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/noise-data-set/noise_test'\n",
    "Out_Noise_data,min_speech_length, min_path = find_min_speech_length_and_path(root_folder,Out_Noise_data)\n",
    "\n",
    "if min_path:\n",
    "    print(f\"The minimum speech length is {min_speech_length} milliseconds, found in file: {min_path}\")\n",
    "else:\n",
    "    print(\"No .wav files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:43.086341Z",
     "iopub.status.busy": "2024-03-02T00:03:43.085925Z",
     "iopub.status.idle": "2024-03-02T00:03:43.092575Z",
     "shell.execute_reply": "2024-03-02T00:03:43.091647Z",
     "shell.execute_reply.started": "2024-03-02T00:03:43.086308Z"
    }
   },
   "outputs": [],
   "source": [
    "len(Out_Noise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:03:43.094014Z",
     "iopub.status.busy": "2024-03-02T00:03:43.093702Z",
     "iopub.status.idle": "2024-03-02T00:06:54.914024Z",
     "shell.execute_reply": "2024-03-02T00:06:54.913009Z",
     "shell.execute_reply.started": "2024-03-02T00:03:43.09399Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "import random\n",
    "import librosa\n",
    "import numpy as np\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "def find_max_speech_length(root_folder, Destination, Out_Noise_data):\n",
    "    max_length = 0\n",
    "\n",
    "    # Loop over all folders and files in the root_folder\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.flac'):\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "\n",
    "                # Replace 'output.wav' with the desired path and name for the output WAV file\n",
    "                output_file = \"/kaggle/working/sample.wav\"\n",
    "                # Open the FLAC file and convert it to WAV\n",
    "                # Read the FLAC audio file             \n",
    "                data, current_sample_rate = sf.read(file_path)\n",
    "                 \n",
    "                #break\n",
    "                sf.write(output_file, data, current_sample_rate)\n",
    "\n",
    "                AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "                Audio = AudioSegment.from_wav(output_file)\n",
    "                Noise =  Out_Noise_data[random.randrange(0, 179)]\n",
    "                audio_clip= None\n",
    "                if(len(Noise) > len(Audio)):\n",
    "                    audio_clip = insert_audio_clip(Noise[:len(Audio)], Audio)\n",
    "                else:\n",
    "                    repeat_factor = int(np.ceil((len(Audio)+0.0) / len(Noise)))\n",
    "                    # Repeat the noise\n",
    "                    RepeatedNoise = Noise * repeat_factor\n",
    "                    # Trim the repeated noise to match the length of the audio\n",
    "                    audio_clip = insert_audio_clip(RepeatedNoise[:len(Audio)], Audio)\n",
    "\n",
    "                output_path = os.path.join(Destination, file_path[40:-5] + '.wav')\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create directories if they don't exist\n",
    "                audio_clip.export(output_path, format=\"wav\")\n",
    "\n",
    "\n",
    "\n",
    "    return max_length\n",
    "\n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/libri-stutter/LibriStutter Audio'\n",
    "Destination = '/kaggle/working/LibriStutter_Data/'\n",
    "max_speech_length = find_max_speech_length(root_folder, Destination, Out_Noise_data)\n",
    "\n",
    "\n",
    "print(f\"The maximum speech length is {max_speech_length} milliseconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install more_itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:06:54.918033Z",
     "iopub.status.busy": "2024-03-02T00:06:54.917682Z",
     "iopub.status.idle": "2024-03-02T00:06:54.922708Z",
     "shell.execute_reply": "2024-03-02T00:06:54.921792Z",
     "shell.execute_reply.started": "2024-03-02T00:06:54.918009Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import lru_cache\n",
    "from subprocess import CalledProcessError, run\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.chdir(\"/kaggle/input/feature-extraction\")\n",
    "from audio import (FRAMES_PER_SECOND,HOP_LENGTH,\n",
    "    N_FRAMES,N_SAMPLES,SAMPLE_RATE, CHUNK_LENGTH, FRAMES_PER_SECOND,\n",
    "    load_audio, exact_div, pad_or_trim, mel_filters, log_mel_spectrogram)\n",
    "os.chdir(\"/kaggle/input/filesa-data-speech/\")\n",
    "from english import EnglishTextNormalizer\n",
    "from english import (EnglishNumberNormalizer,EnglishSpellingNormalizer,)\n",
    "from basic import BasicTextNormalizer as BasicTextNormalizer\n",
    "#!pip install  ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:06:54.924097Z",
     "iopub.status.busy": "2024-03-02T00:06:54.923839Z",
     "iopub.status.idle": "2024-03-02T00:06:54.948115Z",
     "shell.execute_reply": "2024-03-02T00:06:54.947433Z",
     "shell.execute_reply.started": "2024-03-02T00:06:54.924075Z"
    }
   },
   "outputs": [],
   "source": [
    "Norm11=BasicTextNormalizer()\n",
    "Norm12=EnglishTextNormalizer()\n",
    "Norm13=EnglishSpellingNormalizer()\n",
    "Norm14=EnglishNumberNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:07:19.284392Z",
     "iopub.status.busy": "2024-03-02T00:07:19.284115Z",
     "iopub.status.idle": "2024-03-02T00:07:43.677944Z",
     "shell.execute_reply": "2024-03-02T00:07:43.676766Z",
     "shell.execute_reply.started": "2024-03-02T00:07:19.284359Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt update -y && apt upgrade -y\n",
    "!apt install sox libsox-fmt-mp3 -y\n",
    "!apt install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:07:43.679797Z",
     "iopub.status.busy": "2024-03-02T00:07:43.679465Z",
     "iopub.status.idle": "2024-03-02T00:08:09.162813Z",
     "shell.execute_reply": "2024-03-02T00:08:09.162054Z",
     "shell.execute_reply.started": "2024-03-02T00:07:43.679756Z"
    }
   },
   "outputs": [],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "L_Audio=[]\n",
    "L_Txt=[]\n",
    "\n",
    "def find_min_speech_length_and_path(root_folder):\n",
    "    # Loop over all folders and files in the root_folder\n",
    "    io = 0\n",
    "    exi = False\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        print(folder_name)\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n",
    "                io+=1\n",
    "                #print(file_path)\n",
    "                L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                df = pd.read_csv(file_path,header =None)\n",
    "                formatted_line=\"\"\n",
    "                for _, row in df.iterrows():\n",
    "                    text = row[0]\n",
    "                    start_time = row[1]\n",
    "                    end_time = row[2]\n",
    "                    \n",
    "                    if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n",
    "                        formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                    else:\n",
    "                        formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n",
    "                    #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                #with open(file_path, 'r') as file:\n",
    "                #    file_contents = file.read()\n",
    "                L_Txt.append(formatted_line)\n",
    "                if(io == 1000):\n",
    "                    exi = True\n",
    "                    break\n",
    "        if(exi):\n",
    "            break\n",
    "    \n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\n",
    "find_min_speech_length_and_path(root_folder)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:09.164574Z",
     "iopub.status.busy": "2024-03-02T00:08:09.164214Z",
     "iopub.status.idle": "2024-03-02T00:08:09.17341Z",
     "shell.execute_reply": "2024-03-02T00:08:09.172428Z",
     "shell.execute_reply.started": "2024-03-02T00:08:09.164541Z"
    }
   },
   "outputs": [],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "L_Audio=[]\n",
    "L_Txt=[]\n",
    "\n",
    "def find_min_speech_length_and_path(root_folder):\n",
    "    # Loop over all folders and files in the root_folder\n",
    "    io = 0\n",
    "    \n",
    "    exi = False\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        print(folder_name)\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n",
    "                io+=1\n",
    "                if(io > 1000 ):\n",
    "                    #print(file_path)\n",
    "                    L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n",
    "                    file_path = os.path.join(folder_name, filename)\n",
    "                    df = pd.read_csv(file_path,header =None)\n",
    "                    formatted_line=\"\"\n",
    "                    for _, row in df.iterrows():\n",
    "                        text = row[0]\n",
    "                        start_time = row[1]\n",
    "                        end_time = row[2]\n",
    "\n",
    "                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n",
    "                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                        else:\n",
    "                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n",
    "                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                    #with open(file_path, 'r') as file:\n",
    "                    #    file_contents = file.read()\n",
    "                    L_Txt.append(formatted_line)\n",
    "                if(io == 2000):\n",
    "                    exi = True\n",
    "                    break\n",
    "        if(exi):\n",
    "            break\n",
    "    \n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\n",
    "find_min_speech_length_and_path(root_folder)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:09.174872Z",
     "iopub.status.busy": "2024-03-02T00:08:09.174587Z",
     "iopub.status.idle": "2024-03-02T00:08:09.193062Z",
     "shell.execute_reply": "2024-03-02T00:08:09.191853Z",
     "shell.execute_reply.started": "2024-03-02T00:08:09.174849Z"
    }
   },
   "outputs": [],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "L_Audio=[]\n",
    "L_Txt=[]\n",
    "\n",
    "def find_min_speech_length_and_path(root_folder):\n",
    "    # Loop over all folders and files in the root_folder\n",
    "    io = 0\n",
    "    \n",
    "    exi = False\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        print(folder_name)\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n",
    "                io+=1\n",
    "                if(io > 2000 ):\n",
    "                    #print(file_path)\n",
    "                    L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n",
    "                    file_path = os.path.join(folder_name, filename)\n",
    "                    df = pd.read_csv(file_path,header =None)\n",
    "                    formatted_line=\"\"\n",
    "                    for _, row in df.iterrows():\n",
    "                        text = row[0]\n",
    "                        start_time = row[1]\n",
    "                        end_time = row[2]\n",
    "\n",
    "                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n",
    "                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                        else:\n",
    "                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n",
    "                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                    #with open(file_path, 'r') as file:\n",
    "                    #    file_contents = file.read()\n",
    "                    L_Txt.append(formatted_line)\n",
    "                if(io == 3000):\n",
    "                    exi = True\n",
    "                    break\n",
    "        if(exi):\n",
    "            break\n",
    "    \n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\n",
    "find_min_speech_length_and_path(root_folder)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:09.195264Z",
     "iopub.status.busy": "2024-03-02T00:08:09.194901Z",
     "iopub.status.idle": "2024-03-02T00:08:09.209367Z",
     "shell.execute_reply": "2024-03-02T00:08:09.208353Z",
     "shell.execute_reply.started": "2024-03-02T00:08:09.195233Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "L_Audio=[]\n",
    "L_Txt=[]\n",
    "\n",
    "def find_min_speech_length_and_path(root_folder):\n",
    "    # Loop over all folders and files in the root_folder\n",
    "    io = 0\n",
    "    \n",
    "    exi = False\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        print(folder_name)\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n",
    "                io+=1\n",
    "                if(io > 3000 ):\n",
    "                    #print(file_path)\n",
    "                    L_Audio.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n",
    "                    file_path = os.path.join(folder_name, filename)\n",
    "                    df = pd.read_csv(file_path,header =None)\n",
    "                    formatted_line=\"\"\n",
    "                    for _, row in df.iterrows():\n",
    "                        text = row[0]\n",
    "                        start_time = row[1]\n",
    "                        end_time = row[2]\n",
    "\n",
    "                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n",
    "                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                        else:\n",
    "                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n",
    "                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                    #with open(file_path, 'r') as file:\n",
    "                    #    file_contents = file.read()\n",
    "                    L_Txt.append(formatted_line)\n",
    "                if(io == 4000):\n",
    "                    exi = True\n",
    "                    break\n",
    "        if(exi):\n",
    "            break\n",
    "    \n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\n",
    "find_min_speech_length_and_path(root_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:09.210921Z",
     "iopub.status.busy": "2024-03-02T00:08:09.210595Z",
     "iopub.status.idle": "2024-03-02T00:08:25.731507Z",
     "shell.execute_reply": "2024-03-02T00:08:25.730081Z",
     "shell.execute_reply.started": "2024-03-02T00:08:09.210895Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "AudioSegment.converter = \"/path/to/ffmpeg\"\n",
    "L_Audio_val=[]\n",
    "L_Txt_val=[]\n",
    "\n",
    "def find_min_speech_length_and_path(root_folder):\n",
    "    # Loop over all folders and files in the root_folder\n",
    "    io = 0\n",
    "    \n",
    "    exi = False\n",
    "    for folder_name, subfolders, filenames in os.walk(root_folder):\n",
    "        print(folder_name)\n",
    "        for filename in filenames:\n",
    "            if filename.lower().endswith('.csv'):\n",
    "                file_path = '/kaggle/working/LibriStutter_Data/ Audio'+folder_name[52:]+'/'+filename[:-3]+'wav'\n",
    "                io+=1\n",
    "                if(io > 4000 ):\n",
    "                    #print(file_path)\n",
    "                    L_Audio_val.append(pad_or_trim(log_mel_spectrogram(file_path, 80, padding=N_SAMPLES), N_FRAMES))\n",
    "                    file_path = os.path.join(folder_name, filename)\n",
    "                    df = pd.read_csv(file_path,header =None)\n",
    "                    formatted_line=\"\"\n",
    "                    for _, row in df.iterrows():\n",
    "                        text = row[0]\n",
    "                        start_time = row[1]\n",
    "                        end_time = row[2]\n",
    "\n",
    "                        if start_time < 0 or start_time > 30 or round(start_time * 100) % 2 != 0 or end_time < 0 or end_time > 30 or round(end_time * 100) % 2 != 0:\n",
    "                            formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                        else:\n",
    "                            formatted_line += f\"<|{start_time:.2f}|> {Norm14(Norm13(Norm12(Norm11(text))))}<|{end_time:.2f}|>\"\n",
    "                        #formatted_line +=Norm14(Norm13(Norm12(Norm11(text))))\n",
    "                    #with open(file_path, 'r') as file:\n",
    "                    #    file_contents = file.read()\n",
    "                    L_Txt_val.append(formatted_line)\n",
    "                if(io == 4500):\n",
    "                    exi = True\n",
    "                    break\n",
    "        if(exi):\n",
    "            break\n",
    "    \n",
    "# Replace '/path/to/root/folder' with the actual path to your root folder\n",
    "root_folder = '/kaggle/input/libri-stutter/LibriStutter Annotations'\n",
    "find_min_speech_length_and_path(root_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:25.733299Z",
     "iopub.status.busy": "2024-03-02T00:08:25.732919Z",
     "iopub.status.idle": "2024-03-02T00:08:25.739836Z",
     "shell.execute_reply": "2024-03-02T00:08:25.73875Z",
     "shell.execute_reply.started": "2024-03-02T00:08:25.733268Z"
    }
   },
   "outputs": [],
   "source": [
    "print('len(L_Audio) -> ', len(L_Audio))\n",
    "print('len(L_Audio_val) -> ',len(L_Audio_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:25.741135Z",
     "iopub.status.busy": "2024-03-02T00:08:25.740874Z",
     "iopub.status.idle": "2024-03-02T00:08:25.754702Z",
     "shell.execute_reply": "2024-03-02T00:08:25.753796Z",
     "shell.execute_reply.started": "2024-03-02T00:08:25.74111Z"
    }
   },
   "source": [
    "# Tokenization and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:25.756232Z",
     "iopub.status.busy": "2024-03-02T00:08:25.755907Z",
     "iopub.status.idle": "2024-03-02T00:08:25.765609Z",
     "shell.execute_reply": "2024-03-02T00:08:25.764814Z",
     "shell.execute_reply.started": "2024-03-02T00:08:25.756201Z"
    }
   },
   "outputs": [],
   "source": [
    "def exact_div(x, y):\n",
    "    assert x % y == 0\n",
    "    return x // y\n",
    "\n",
    "\n",
    "# hard-coded audio hyperparameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30 # 30-second\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
    "N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
    "N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
    "FRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
    "TOKENS_PER_SECOND = exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:25.767252Z",
     "iopub.status.busy": "2024-03-02T00:08:25.766681Z",
     "iopub.status.idle": "2024-03-02T00:08:49.427773Z",
     "shell.execute_reply": "2024-03-02T00:08:49.42684Z",
     "shell.execute_reply.started": "2024-03-02T00:08:25.767228Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install more_itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:49.429522Z",
     "iopub.status.busy": "2024-03-02T00:08:49.429222Z",
     "iopub.status.idle": "2024-03-02T00:08:49.62703Z",
     "shell.execute_reply": "2024-03-02T00:08:49.626246Z",
     "shell.execute_reply.started": "2024-03-02T00:08:49.429495Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/input/tokenizer\")\n",
    "from tokenizer import Tokenizer, get_tokenizer\n",
    "os.chdir(\"/kaggle/input\")\n",
    "tokenizer =  get_tokenizer()\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import  List, Optional, Tuple, Union\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:49.628749Z",
     "iopub.status.busy": "2024-03-02T00:08:49.628356Z",
     "iopub.status.idle": "2024-03-02T00:08:49.63541Z",
     "shell.execute_reply": "2024-03-02T00:08:49.634507Z",
     "shell.execute_reply.started": "2024-03-02T00:08:49.628716Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.eot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:08:49.637176Z",
     "iopub.status.busy": "2024-03-02T00:08:49.636908Z",
     "iopub.status.idle": "2024-03-02T00:08:49.664142Z",
     "shell.execute_reply": "2024-03-02T00:08:49.663174Z",
     "shell.execute_reply.started": "2024-03-02T00:08:49.637149Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        mels,\n",
    "        records,\n",
    "        tokenizer: Tokenizer,\n",
    "        fp16: bool = True,\n",
    "        no_timestamps_training: bool = False,\n",
    "        max_prompt_length: int = 223,  # The maximum number of tokens to use for the prompt\n",
    "        prompt_use_rate: float = 0.5,\n",
    "        no_timestamps_rate: float = 0.5,\n",
    "    ) -> None:\n",
    "        self.mels = mels\n",
    "        self.records = records\n",
    "        self.tokenizer = tokenizer\n",
    "        self.fp16 = fp16\n",
    "        self.no_timestamps_training = no_timestamps_training\n",
    "        self.max_prompt_length = max_prompt_length\n",
    "        self.prompt_use_rate = prompt_use_rate\n",
    "        self.no_timestamps_rate = no_timestamps_rate\n",
    "\n",
    "        self.num_frames_per_second = N_FRAMES / CHUNK_LENGTH\n",
    "        # timestamps tokens are from <|0.00|> to <|30.00|> with a step of 0.02\n",
    "        self.timestamp_pattern = re.compile(r\"(<\\|[123]?[0-9]\\.[0-9][0-9]\\|>)\")\n",
    "        self.model_n_text_ctx = 448\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def _get_prompt_tokens(self, prompt: str) -> List[int]:\n",
    "        if len(prompt) > 0 and torch.rand(1) < self.prompt_use_rate:\n",
    "            prompt_tokens = self._encode_text_with_timestamps(prompt)[-self.max_prompt_length :]\n",
    "            prompt_tokens = [self.tokenizer.sot_prev] + prompt_tokens\n",
    "        else:\n",
    "            prompt_tokens = []\n",
    "\n",
    "        return prompt_tokens\n",
    "\n",
    "    def _get_special_tokens(\n",
    "        self, is_text_empty: bool, language: str, no_timestamps: bool\n",
    "    ) -> List[int]:\n",
    "        if is_text_empty:\n",
    "            special_tokens = [self.tokenizer.sot, self.tokenizer.no_speech]\n",
    "        else:\n",
    "            special_tokens = [\n",
    "                self.tokenizer.sot,\n",
    "                self.tokenizer.special_tokens[f\"<|{language}|>\"],\n",
    "                self.tokenizer.special_tokens[\"<|transcribe|>\"],\n",
    "            ]\n",
    "            if no_timestamps:\n",
    "                special_tokens.append(self.tokenizer.no_timestamps)\n",
    "\n",
    "        return special_tokens\n",
    "\n",
    "    def _encode_text_with_timestamps(self, text: str) -> List[int]:\n",
    "        parts = self.timestamp_pattern.split(text)\n",
    "        parts = [token for token in parts if token != \"\"]\n",
    "        tokens = []\n",
    "        for part in parts:\n",
    "            if self.timestamp_pattern.fullmatch(part) is not None:\n",
    "                timestamp = float(part[2:-2])\n",
    "\n",
    "                # timestamp must be in the range [0, 30] and be a multiple of 0.02 seconds\n",
    "                if timestamp < 0 or timestamp > 30 or round(timestamp * 100) % 2 != 0:\n",
    "                    raise ValueError(f\"Invalid timestamp: {timestamp}\")\n",
    "\n",
    "                token = self.tokenizer.timestamp_begin + round(timestamp * 100) // 2\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(self.tokenizer.encode(part))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _get_partial_segment_start(self, tokens: List[int]) -> Optional[float]:\n",
    "        if (\n",
    "            len(tokens) >= 2\n",
    "            and tokens[-2] >= self.tokenizer.timestamp_begin\n",
    "            and tokens[-1] >= self.tokenizer.timestamp_begin\n",
    "        ):  # if the last token is a start time token\n",
    "            return (tokens[-1] - self.tokenizer.timestamp_begin) * 0.02\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _get_text_tokens(self, text: str, no_timestamps: bool) -> Tuple[List[int], Optional[float]]:\n",
    "        text_tokens = self._encode_text_with_timestamps(text)\n",
    "        next_partial_segment_start = self._get_partial_segment_start(text_tokens)\n",
    "        if no_timestamps:\n",
    "            text_tokens = list(filter(lambda x: x < self.tokenizer.timestamp_begin, text_tokens))\n",
    "\n",
    "        return text_tokens, next_partial_segment_start\n",
    "\n",
    "    def _calculate_mel(\n",
    "        self, audio_path: str, next_partial_segment_start: Optional[float], no_timestamps: bool\n",
    "    ) -> torch.Tensor:\n",
    "        mel = log_mel_spectrogram(audio_path)\n",
    "        if no_timestamps and next_partial_segment_start is not None:\n",
    "            mel = mel[:, : int(next_partial_segment_start * self.num_frames_per_second)]\n",
    "        mel = pad_or_trim(mel, N_FRAMES)\n",
    "        if self.fp16:\n",
    "            mel = mel.half()\n",
    "\n",
    "        return mel\n",
    "\n",
    "    def _construct_decoder_output(\n",
    "        self, prompt_tokens: List[int], special_tokens: List[int], text_tokens: List[int]\n",
    "    ) -> List[int]:\n",
    "        if len(prompt_tokens) == 0:\n",
    "            decoder_output = special_tokens[1:] + text_tokens + [self.tokenizer.eot]\n",
    "        else:\n",
    "            decoder_output = (\n",
    "                # Mask out the training loss for predicting the prompt tokens. We use \"-100\" as the\n",
    "                # default value for the `ignore_index` parameter in\n",
    "                # `torch.nn.functional.cross_entropy()`. However, we do not mask out the loss for\n",
    "                # predicting the sot token because our experiment indicates that the original\n",
    "                # Whisper model assigns a high probability to the sot token after prompt tokens.\n",
    "                [-100] * (len(prompt_tokens) - 1)\n",
    "                + special_tokens\n",
    "                + text_tokens\n",
    "                + [self.tokenizer.eot]\n",
    "            )\n",
    "        return decoder_output\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        record = self.records[index]\n",
    "        mel = self.mels[index]\n",
    "        no_timestamps = self.no_timestamps_training\n",
    "        #no_timestamps = True\n",
    "        #print(no_timestamps)\n",
    "        text_tokens, next_partial_segment_start = self._get_text_tokens(record, no_timestamps)\n",
    "        is_text_empty = len(text_tokens) == 0\n",
    "        special_tokens = self._get_special_tokens(is_text_empty, \"en\", no_timestamps)\n",
    "\n",
    "        decoder_input = special_tokens + text_tokens\n",
    "        if len(decoder_input) > self.model_n_text_ctx:\n",
    "            raise ValueError(f\"Input is too long: {record} (length: {len(decoder_input)})\")\n",
    "\n",
    "        decoder_output = self._construct_decoder_output([],special_tokens, text_tokens)\n",
    "\n",
    "        \n",
    "        return (\n",
    "            mel,\n",
    "            torch.tensor(decoder_input, dtype=torch.long),\n",
    "            torch.tensor(decoder_output, dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:19.392644Z",
     "iopub.status.busy": "2024-03-02T00:18:19.392217Z",
     "iopub.status.idle": "2024-03-02T00:18:19.403688Z",
     "shell.execute_reply": "2024-03-02T00:18:19.402628Z",
     "shell.execute_reply.started": "2024-03-02T00:18:19.392605Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate_fn(data):\n",
    "    x, y_in, y_out = zip(*data)\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y_in = pad_sequence(y_in, batch_first=True, padding_value=50256)#eot token\n",
    "    y_out = pad_sequence(y_out, batch_first=True, padding_value=-100)\n",
    "    return x, y_in, y_out\n",
    "tokenizer =  get_tokenizer()\n",
    "dataset = AudioDataset(\n",
    "    L_Audio,\n",
    "    L_Txt,\n",
    "    tokenizer,\n",
    "    no_timestamps_training=False)\n",
    "DataLoaders = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "dataset_val = AudioDataset(\n",
    "    L_Audio_val,\n",
    "    L_Txt_val,\n",
    "    tokenizer,\n",
    "    no_timestamps_training=False)\n",
    "DataLoaders_val = DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=4,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:19.727181Z",
     "iopub.status.busy": "2024-03-02T00:18:19.726349Z",
     "iopub.status.idle": "2024-03-02T00:18:19.73238Z",
     "shell.execute_reply": "2024-03-02T00:18:19.731489Z",
     "shell.execute_reply.started": "2024-03-02T00:18:19.727148Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir(\"/kaggle/input/whispersa\")\n",
    "from model import (Whispersa,TextDecoder,AudioEncoder,ModelDimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:21.851304Z",
     "iopub.status.busy": "2024-03-02T00:18:21.850577Z",
     "iopub.status.idle": "2024-03-02T00:18:21.855876Z",
     "shell.execute_reply": "2024-03-02T00:18:21.855002Z",
     "shell.execute_reply.started": "2024-03-02T00:18:21.851271Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:21.876717Z",
     "iopub.status.busy": "2024-03-02T00:18:21.876443Z",
     "iopub.status.idle": "2024-03-02T00:18:21.881755Z",
     "shell.execute_reply": "2024-03-02T00:18:21.880743Z",
     "shell.execute_reply.started": "2024-03-02T00:18:21.876693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of ModelDimensions\n",
    "dimensions_instance = ModelDimensions(\n",
    "    n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=tokenizer.encoding.n_vocab, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12\n",
    "   # n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=51864, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:21.929294Z",
     "iopub.status.busy": "2024-03-02T00:18:21.928753Z",
     "iopub.status.idle": "2024-03-02T00:18:54.48606Z",
     "shell.execute_reply": "2024-03-02T00:18:54.484826Z",
     "shell.execute_reply.started": "2024-03-02T00:18:21.929269Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "import whisper\n",
    "modela = whisper.load_model('small.en').to(device)\n",
    "model = Whispersa(dimensions_instance).to(device)\n",
    "model.load_state_dict(modela.state_dict())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:54.488288Z",
     "iopub.status.busy": "2024-03-02T00:18:54.487962Z",
     "iopub.status.idle": "2024-03-02T00:18:54.494306Z",
     "shell.execute_reply": "2024-03-02T00:18:54.493341Z",
     "shell.execute_reply.started": "2024-03-02T00:18:54.488258Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "model = Whispersa(dimensions_instance).to(device)\n",
    "save_path = \"/kaggle/input/speech-pro1/model2.pt\"  # Replace with your desired path\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:54.496418Z",
     "iopub.status.busy": "2024-03-02T00:18:54.495665Z",
     "iopub.status.idle": "2024-03-02T00:18:54.508742Z",
     "shell.execute_reply": "2024-03-02T00:18:54.507922Z",
     "shell.execute_reply.started": "2024-03-02T00:18:54.496383Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion,device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "         for batch in iterator:\n",
    "            x, y_in, y_out = batch\n",
    "            y_in,x, y_out = y_in.to(device), x.to(device), y_out.to(device)\n",
    "            audio_features = model.embed_audio(x)  \n",
    "            logits = model.logits(y_in, audio_features=audio_features)\n",
    "            loss = loses(logits.view(-1, logits.size(-1)), y_out.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:54.51115Z",
     "iopub.status.busy": "2024-03-02T00:18:54.510817Z",
     "iopub.status.idle": "2024-03-02T00:18:54.520813Z",
     "shell.execute_reply": "2024-03-02T00:18:54.519936Z",
     "shell.execute_reply.started": "2024-03-02T00:18:54.511119Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_step(model,train_iter,optimizer: torch.optim.Optimizer,accum_grad_steps: int,train_only_decoder: bool,loses,device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for _ in range(accum_grad_steps):\n",
    "        x, y_in, y_out = next(train_iter)\n",
    "        y_in,x, y_out = y_in.to(device), x.to(device), y_out.to(device)\n",
    "        if train_only_decoder:\n",
    "            with torch.no_grad():\n",
    "                audio_features = model.embed_audio(x)\n",
    "        else:\n",
    "            audio_features = model.embed_audio(x)        \n",
    "        \n",
    "        logits = model.logits(y_in, audio_features=audio_features)\n",
    "        #loss = F.cross_entropy(logits.transpose(1, 2), y_out)\n",
    "        #print(logits.shape)\n",
    "        #print(logits.view(-1, logits.shape[-1]).shape)\n",
    "        #print( logits.shape[-1])\n",
    "        loss = loses(logits.view(-1, logits.size(-1)), y_out.view(-1))\n",
    "        #print(logits.view(-1, tokenizer.encoding.n_vocab).shape)\n",
    "        #loss = F.cross_entropy(logits.view(-1, tokenizer.encoding.n_vocab), y_out)\n",
    "        loss = loss / accum_grad_steps\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T00:18:54.522018Z",
     "iopub.status.busy": "2024-03-02T00:18:54.521715Z",
     "iopub.status.idle": "2024-03-02T00:21:17.489499Z",
     "shell.execute_reply": "2024-03-02T00:21:17.488192Z",
     "shell.execute_reply.started": "2024-03-02T00:18:54.521989Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "loses = torch.nn.CrossEntropyLoss(ignore_index=-100).to(device)\n",
    "#loses = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "def infinite_iter(data_loader: DataLoader):\n",
    "    while True:\n",
    "        for batch in data_loader:\n",
    "            yield batch\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(range(1, 91))\n",
    "train_iter = infinite_iter(DataLoaders)\n",
    "for step in pbar:\n",
    "    train_loss =train_step(model,train_iter,optimizer,64,False,loses,device)\n",
    "    val_loss =evaluate(model,DataLoaders_val,loses,device)\n",
    "    pbar.set_postfix({\"loss\": train_loss,\"val loss\": val_loss})\n",
    "    print(\"step -> \", step,\" train_loss = \", train_loss,\" val_loss = \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def remove_path_and_contents(path):\n",
    "    try:\n",
    "        # Remove the entire path and its contents\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Removed path and its contents: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Replace 'your_path_here' with the actual path you want to remove\n",
    "path_to_remove = '/kaggle/working/LibriStutter_Data'\n",
    "remove_path_and_contents(path_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the saving path\n",
    "save_pasth = \"/kaggle/working/model3.pt\"  # Replace with your desired path\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), save_pasth)\n",
    "\n",
    "print(f\"Model saved successfully to: {save_pasth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "models = Whispersa(dimensions_instance).to(device)\n",
    "save_path = \"/kaggle/input/speech-pro1/model.pt\"  # Replace with your desired path\n",
    "models.load_state_dict(torch.load(save_path))\n",
    "# Define the saving path\n",
    "save_pasth = \"/kaggle/working/model.pt\"  # Replace with your desired path\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(models.state_dict(), save_pasth)\n",
    "\n",
    "print(f\"Model saved successfully to: {save_pasth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "modelsa = Whispersa(dimensions_instance).to(device)\n",
    "save_path = \"/kaggle/input/speech-pro1/model1.pt\"  # Replace with your desired path\n",
    "modelsa.load_state_dict(torch.load(save_path))\n",
    "# Define the saving path\n",
    "save_pasth = \"/kaggle/working/model1.pt\"  # Replace with your desired path\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(modelsa.state_dict(), save_pasth)\n",
    "\n",
    "print(f\"Model saved successfully to: {save_pasth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "modelsaa = Whispersa(dimensions_instance).to(device)\n",
    "save_path = \"/kaggle/input/speech-pro1/model2.pt\"  # Replace with your desired path\n",
    "modelsaa.load_state_dict(torch.load(save_path))\n",
    "# Define the saving path\n",
    "save_pasth = \"/kaggle/working/model2.pt\"  # Replace with your desired path\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(modelsaa.state_dict(), save_pasth)\n",
    "\n",
    "print(f\"Model saved successfully to: {save_pasth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Inference to predict the output After training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:16:36.162875Z",
     "iopub.status.busy": "2024-03-02T21:16:36.162488Z",
     "iopub.status.idle": "2024-03-02T21:17:06.541221Z",
     "shell.execute_reply": "2024-03-02T21:17:06.540084Z",
     "shell.execute_reply.started": "2024-03-02T21:16:36.162815Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "os.chdir(\"/kaggle/input/whispersa\")\n",
    "from model import (Whispersa,TextDecoder,AudioEncoder,ModelDimensions)\n",
    "!pip install tiktoken\n",
    "!pip install more_itertools\n",
    "import os\n",
    "os.chdir(\"/kaggle/input/tokenizer\")\n",
    "from tokenizer import Tokenizer, get_tokenizer\n",
    "tokenizer =  get_tokenizer()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:17:06.543831Z",
     "iopub.status.busy": "2024-03-02T21:17:06.543267Z",
     "iopub.status.idle": "2024-03-02T21:17:16.172048Z",
     "shell.execute_reply": "2024-03-02T21:17:16.171081Z",
     "shell.execute_reply.started": "2024-03-02T21:17:06.543788Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Create an instance of ModelDimensions\n",
    "dimensions_instance = ModelDimensions(\n",
    "       n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=tokenizer.encoding.n_vocab, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12\n",
    "\n",
    ")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\"cpu\"\n",
    "model = Whispersa(dimensions_instance).to(device)\n",
    "save_path = \"/kaggle/input/speech-pro1/model1.pt\"  # Replace with your desired path\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.to(device)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model.device'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:17:22.559136Z",
     "iopub.status.busy": "2024-03-02T21:17:22.558727Z",
     "iopub.status.idle": "2024-03-02T21:17:22.61748Z",
     "shell.execute_reply": "2024-03-02T21:17:22.616428Z",
     "shell.execute_reply.started": "2024-03-02T21:17:22.559103Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import os\n",
    "os.chdir(\"/kaggle/input/transcribe\")\n",
    "from transcribe import transcribe'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T21:17:22.905565Z",
     "iopub.status.busy": "2024-03-02T21:17:22.904991Z",
     "iopub.status.idle": "2024-03-02T21:17:34.433681Z",
     "shell.execute_reply": "2024-03-02T21:17:34.432462Z",
     "shell.execute_reply.started": "2024-03-02T21:17:22.905536Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "result = transcribe(model, '/kaggle/input/libri-stutter/LibriStutter Audio/445/123857/445-123857-0000.flac', language='en', temperature=1.0, word_timestamps=True)\n",
    "print(result['text'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "!pip install more_itertools\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/input/whispersa\")\n",
    "from model import (Whispersa,TextDecoder,AudioEncoder,ModelDimensions)\n",
    "os.chdir(\"/kaggle/input/tokenizer\")\n",
    "from tokenizer import Tokenizer, get_tokenizer\n",
    "tokenizer =  get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of ModelDimensions\n",
    "dimensions_instance = ModelDimensions(\n",
    "       n_mels=80, n_audio_ctx=1500, n_audio_state=768, n_audio_head=12, n_audio_layer=12, n_vocab=tokenizer.encoding.n_vocab, n_text_ctx=448, n_text_state=768, n_text_head=12, n_text_layer=12\n",
    "\n",
    ")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\"cpu\"\n",
    "\n",
    "model = Whispersa(dimensions_instance).to(device)\n",
    "save_path = \"/kaggle/input/speech-pro1/model3.pt\"  # Replace with your desired path\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/kaggle/input/transcribe\")\n",
    "from transcribe import transcribe\n",
    "import torchaudio\n",
    "dataset = torchaudio.datasets.LIBRISPEECH( root=os.path.expanduser(\"~/.cache\"),url=\"test-clean\", download=True  )\n",
    "\n",
    "beam_size=5\n",
    "temperature=(0.6, 0.8, 1.0)\n",
    "decode_options = dict( beam_size=beam_size, temperature=temperature,patience=2)\n",
    "transcribe_options = dict(task=\"transcribe\", **decode_options)\n",
    "hypotheses=[]\n",
    "references = []\n",
    "def Calc_Predict():\n",
    "    for i in range(len(dataset)):\n",
    "        audio, sample_rate, text, _, _, _ = dataset[i]\n",
    "        result = transcribe(model, audio[0], language='en', word_timestamps=True)#, **transcribe_options)\n",
    "        hypotheses.append(result['text'])\n",
    "        references.append(text)\n",
    "        \n",
    "Calc_Predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "data = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
    "data\n",
    "import jiwer\n",
    "os.chdir(\"/kaggle/input/filesa-data-speech/\")\n",
    "from english import EnglishTextNormalizer\n",
    "normalizer = EnglishTextNormalizer()\n",
    "data[\"hypothesis_clean\"] = [normalizer(text) for text in data[\"hypothesis\"]]\n",
    "data[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\n",
    "\n",
    "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "print(f\"WER: {wer * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4143770,
     "sourceId": 7171840,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4143772,
     "sourceId": 7171843,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4143785,
     "sourceId": 7171864,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4148047,
     "sourceId": 7177548,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4148135,
     "sourceId": 7177667,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4486033,
     "sourceId": 7687455,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4486106,
     "sourceId": 7687555,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4522074,
     "sourceId": 7737409,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4522099,
     "sourceId": 7737443,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4522107,
     "sourceId": 7737452,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 165199443,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
